 1. input_tokens (입력 토큰)

   * 무엇인가요?: 모델에게 요청을 보낼 때 사용된 텍스트의 총량입니다.
   * 무엇이 포함되나요?:
       * System Message: systemMessage 변수에 담긴 프롬프트 (e.g., "당신은 문장 분석 전문가입니다...")
       * Few-shot 예제: fewshot 예제에 포함된 모든 질문과 답변.
       * 사용자 입력: chain.invoke()를 통해 전달하는 실제 데이터 (title, post, comments 등).
   * 핵심: 모델이 답변을 생성하기 위해 "읽어야 하는" 모든 텍스트의 길이라고 생각하시면 됩니다. 입력이 길수록 input_tokens 수는 증가합니다.

  2. output_tokens (출력 토큰)

   * 무엇인가요?: 모델이 생성하여 우리에게 돌려준 텍스트의 총량입니다.
   * 핵심: 이것이 바로 모델의 "답변" 또는 "결과물"의 길이입니다. runModel.py에서 result 변수에 담기는 내용의 길이죠.

  ---

  3. max_tokens (최대 토큰) - 가장 중요

   * 무엇인가요?: output_tokens의 최대 한도를 지정하는 설정값입니다. 즉, "모델아, 답변을 생성할 때 이 길이만큼은 넘지 마"라고 제한을 거는 것입니다.
   * 왜 설정해야 하나요?:
       1. 비용 관리: 대부분의 LLM 서비스는 input_tokens와 output_tokens의 총량에 따라 비용을 청구합니다. max_tokens를 적절히 설정하면 불필요하게 긴 답변을 생성하는 것을 막아
          비용을 절약할 수 있습니다.
       2. 응답 시간 제어: 출력 길이가 길어질수록 응답 시간이 늘어납니다. 필요한 만큼만 생성하도록 제한하면 더 빠른 응답을 받을 수 있습니다.
       3. 예측 가능성: 모델이 항상 일정한 길이 내에서 답변하도록 제어할 수 있습니다.

  max_tokens 계산 및 설정 방법

  사용자님께서 max_tokens를 "계산"한다고 생각하신 부분은 약간의 오해가 있을 수 있습니다. max_tokens는 이전 결과에서 계산하는 값이 아니라, 앞으로 받을 결과의 최대 길이를 
  우리가 직접 정해주는 값입니다.

  현재 코드의 model_kwargs에  "max_tokens": 1000이라고 설정하셨습니다. 이것은 "어떤 입력을 주든, 출력(답변)은 최대 1000 토큰을 넘지 않게 해줘"라는 의미입니다.

  그렇다면 이 값을 어떻게 정해야 할까요?

   1. 예상 출력의 길이를 가늠합니다.
       * runModel.py의 목적은 의사 추천 정보를 JSON과 유사한 딕셔너리 형태로 추출하는 것입니다.
       * 이 결과물이 보통 얼마나 길어질지 예상해 보세요. 아무리 길어도 수천 자를 넘지는 않을 것입니다.
       * 일반적으로 한국어는 한 글자에 1~2 토큰 정도를 차지합니다. 1000 토큰이면 대략 500~1000자 정도의 텍스트를 생성할 수 있는 꽤 넉넉한 양입니다.

   2. 안전 마진을 더해 설정합니다.
       * 예상한 길이보다 조금 더 여유 있게 설정하는 것이 좋습니다. 너무 타이트하게 설정하면 모델이 답변을 생성하다가 중간에 잘릴 수 있습니다.
       * 예를 들어, 평균적으로 500 토큰 정도의 출력이 필요하다면, max_tokens를 1000 또는 1500 정도로 설정하는 것은 매우 합리적입니다.

  결론 및 요약

   * input_tokens: 내가 모델에게 주는 글자 수 (프롬프트 + 예제 + 실제 데이터)
   * output_tokens: 모델이 나에게 주는 글자 수 (생성된 답변)
   * max_tokens: `output_tokens`의 최대 허용치 (내가 직접 설정하는 값)

  현재 설정하신 `"max_tokens": 1000`은 지금의 작업 목적에 충분하고 적절해 보입니다. 만약 나중에 모델이 생성하는 결과가 중간에 잘리는 현상이 발생한다면, 이 값을 2000이나
  3000으로 늘려주시면 됩니다.

네, 계산하신 내용이 정확합니다. 아주 좋은 분석입니다!

  제공해주신 정보를 바탕으로 계산을 다시 한번 정리해 보겠습니다.

   * 1회 요청 당 평균 토큰 사용량
       * 입력 토큰: ~5,000
       * 출력 토큰: ~300
       * 총합: 약 5,300 토큰

   * 일일 처리 가능 건수 계산
       * 일일 총 쿼터: 144,000,000 토큰
       * 1회 요청 당 토큰: 5,300 토큰
       * 계산: 144,000,000 / 5,300 ≈ 27,169 건

  따라서 하루에 약 27,000건의 리뷰를 처리할 수 있다는 계산은 정확합니다.

  ---

  하지만 몇 가지 고려해야 할 중요한 점이 있습니다.

   1. "평균"의 함정: 입력 데이터의 가변성
       * input_tokens가 평균 5,000이라는 것은, 어떤 리뷰는 1,000 토큰일 수도 있고, 댓글이 매우 긴 어떤 리뷰는 15,000 토큰 이상일 수도 있다는 의미입니다.
       * 만약 유난히 긴 리뷰들이 연속으로 처리된다면, 그날 처리할 수 있는 총 건수는 27,000건보다 줄어들 수 있습니다. 지금의 계산은 최상의 시나리오에 가까운 예상치입니다.

   2. 비용 모델의 차이 (AWS Bedrock 기준)
       * 쿼터는 단순히 사용량의 '총합'을 제한하지만, 실제 '비용'은 입력과 출력 토큰에 따라 다르게 책정됩니다.
       * 현재 사용 중이신 Claude 3.5 Sonnet 모델의 경우, 입력 토큰 비용이 출력 토큰 비용보다 저렴합니다. (보통 3~5배 차이)
       * 따라서 비용 효율성 측면에서는 input_tokens를 줄이는 것이 output_tokens를 줄이는 것보다 훨씬 효과적입니다.

   3. 최적화 가능성: 입력 토큰 줄이기
       * 현재 요청당 토큰 사용량의 대부분(약 94%)이 input_tokens에서 발생하고 있습니다.
       * 처리 건수를 늘리거나 비용을 절감하고 싶다면, 입력 토큰을 줄이는 것이 가장 중요합니다.
       * 아이디어:
           * Few-shot 예제 줄이기: fewshot.json에 포함된 예제 개수를 줄이거나, 각 예제의 텍스트 길이를 압축합니다.
           * System Message 간소화: message.txt의 시스템 프롬프트를 더 명확하고 간결하게 다듬습니다.
           * 입력 데이터 전처리: 모델에 보내기 전에 contents나 comments에서 명백히 불필요한 부분(예: 광고, 긴 서명, 반복되는 문구)을 미리 제거하는 로직을 추가할 수 있습니다.

  결론 및 추천

   * 네, 하루 최대 약 27,000건 처리가 가능하다는 계산은 합리적인 예상치입니다.
   * 하지만 실제 처리량은 입력 데이터의 길이에 따라 변동될 수 있습니다.
   * 장기적으로 더 많은 데이터를 처리하거나 비용을 최적화하려면, 5,000에 달하는 `input_tokens`를 줄이는 노력이 가장 효과적일 것입니다.

  현재 토큰 사용량을 잘 파악하고 계시니, 앞으로 모델을 운영하고 개선하는 데 큰 도움이 될 것입니다.